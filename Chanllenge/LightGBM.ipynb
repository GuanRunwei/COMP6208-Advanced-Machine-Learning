{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LightGBM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ouirWwVf7o"
      },
      "source": [
        "# F1 score\n",
        "from sklearn.metrics import f1_score\n",
        "#[1,2,3,2,1,3]\n",
        "#[1,2,3,1,1,3]\n",
        "def cal_macro_f1(y_true,y_pred):\n",
        "    score = f1_score(y_true,y_pred,average='macro')\n",
        "    return score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_eval, y_train1, y_eval = train_test_split(x_train, y_train, test_size=0.2, random_state=2019)\n",
        "\n",
        "# Cross Validation\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=666)\n",
        "train_matrix = np.zeros((df_train.shape[0],2)) #prob for evaluation\n",
        "test_pre_matrix = np.zeros((10,df_test.shape[0],2)) #save test prob\n",
        "cv_scores=[] #cv scores\n",
        "acc_scores=[]\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "for i,(train_index,eval_index) in enumerate(kf.split(x_train)):\n",
        "    print(len(train_index),len(eval_index))\n",
        "    \n",
        "    \n",
        "    #training set\n",
        "    X_train = x_train.iloc[train_index,:]\n",
        "    # y_train1 = df_train.iloc[train_index,-1]\n",
        "    y_train1 = y_train[train_index]\n",
        "    \n",
        "    #validation set\n",
        "    X_eval = x_train.iloc[eval_index,:]\n",
        "    y_eval = y_train[eval_index]\n",
        "    \n",
        "\n",
        "    model =lgb.LGBMClassifier(boosting_type='gbdt', \n",
        "                   num_leaves=2**10,\n",
        "                   max_depth=-1, \n",
        "                   learning_rate= 0.01,\n",
        "                   n_estimators=500,\n",
        "                   objective='binary',\n",
        "                   subsample=0.7,#\n",
        "                   colsample_bytree=0.5,#\n",
        "                   reg_lambda=3,#l2\n",
        "                   n_jobs=16, #\n",
        "                   silent=True, \n",
        "                   random_state=2019,\n",
        "                #    colsample_bylevel=0.5,\n",
        "                   subsample_for_bin=300000,\n",
        "                   min_child_weight=1.5\n",
        "                #    metric='multi_logloss'\n",
        "                  )\n",
        "    model.fit(X_train,y_train1,eval_set=(X_eval,y_eval), early_stopping_rounds=20)\n",
        "    ####predict for validation set\n",
        "    eval_prob = model.predict_proba(X_eval)\n",
        "    eval_pred = np.argmax(eval_prob,axis=1)\n",
        "    score = cal_macro_f1(y_eval,eval_pred)\n",
        "    acc = sum(eval_pred == y_eval) / len(y_eval)\n",
        "\n",
        "    acc_scores.append(acc)\n",
        "    cv_scores.append(score)\n",
        "    print(\"validation F1 score is\",score)\n",
        "    print(\"validation Accuracy score is\",acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}